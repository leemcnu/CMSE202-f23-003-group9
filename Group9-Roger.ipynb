{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f01ec1",
   "metadata": {},
   "source": [
    "# Analyzing the dataset using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dafefd",
   "metadata": {},
   "source": [
    "# Important Note!\n",
    "\n",
    "It is important to acknowledge that we did not learn TensorFlow in class. Therefore, the majority of the code below was generated by AI and modified by me to fit the specific goals of the project. The AI used was CHAT GPT. I will leave a prompt chain and a link to the query below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01471d",
   "metadata": {},
   "source": [
    "TensorFlow has many different ways to analyze a dataset. For my analysis, I will be changing activation functions, learning rates, and epochs to see which combination yields the best results. It is important to note that modifying the parameters can significantly change the amount of computing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac73dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ead70",
   "metadata": {},
   "source": [
    "Please note that TensorFlow was not covered in this class. As a result of that generative AI played a larger role than normal in helping me complete this section of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6814c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class was created with the help of generative AI.\n",
    "# Source OpenAI\n",
    "# URL: https://chat.openai.com/\n",
    "# Parts of the code that were directly generated using chat GPT will be cited directly in the class.\n",
    "\n",
    "\n",
    "class DataAnalyzer():\n",
    "    \"\"\" Data analyzer class.\n",
    "    \n",
    "    Takes a dataset and uses tensor flow to analyze the data and return predictions\n",
    "    \n",
    "    Methods: \n",
    "        __init__(self, data, n_components, learning_rate, activation_function, features, target, epoch=100, verbose=1, test_size=0.2)\n",
    "            Initialize all of the arguments needed to model the dataset with TensorFlow.\n",
    "        \n",
    "        process_data(self, features, target, n_components, test_size)\n",
    "            Processes the given dataset using train_test_split and PCAs\n",
    "            \n",
    "        model(self, pca_train_vectors, pca_test_vectors, y_train_scaled, y_test_scaled)\n",
    "            Models the dataset using TensorFlow.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, n_components, learning_rate, activation_function, features, target, epoch=100, verbose=1, test_size=0.2):\n",
    "        \"\"\"Initialize all of the arguments needed to model the dataset with TensorFlow.\n",
    "        Arguments:\n",
    "        \n",
    "        data [type: Pandas dataframe]\n",
    "            The dataset that will be modeled with TensorFlow\n",
    "            \n",
    "        n_components [type: int]\n",
    "            The amount of feature vectors you want to use with the PCA.\n",
    "                \n",
    "        learning_rate [type: float]\n",
    "            The learning rate of the model.\n",
    "                \n",
    "        activation_function [type: string]\n",
    "            The type of activation function that will anaylze the data.\n",
    "                \n",
    "        features [type: Pandas dataframe]\n",
    "            The dataframe of features you would like to use to train and test the model.\n",
    "                \n",
    "        target [type: Pandas dataframe]\n",
    "            The variable in the dataframe tha you would like to predict.\n",
    "                \n",
    "        epoch [type: int]\n",
    "            The amount of times you would like the model to run. Higher epochs correspond with longer runtimes.\n",
    "                \n",
    "        verbose [type: int]\n",
    "            Adjust the visual output of the model when ran.\n",
    "                \n",
    "        test_size [type: float]\n",
    "            Scale the size of the training and testing sets.\"\"\"\n",
    "        \n",
    "        # Initialize the input variables\n",
    "        self.data = data\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_function = activation_function\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.test_size = test_size\n",
    "        self.n_components = n_components\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "\n",
    "    def process_data(self, features, target, n_components, test_size):\n",
    "        \n",
    "        \"\"\"Processes the given dataset using train_test_split and PCAs.\n",
    "        Arguments:\n",
    "        \n",
    "            features [type: Pandas dataframe]\n",
    "                The dataframe of features you would like to use to train and test the model.\n",
    "                \n",
    "            target [type: Pandas dataframe]\n",
    "                The variable in the dataframe tha you would like to predict.\n",
    "                \n",
    "            n_components [type: int]\n",
    "                The amount of feature vectors you want to use with the PCA.\n",
    "                \n",
    "            test_size [type: float]\n",
    "                Scale the size of the training and testing sets. \"\"\"\n",
    "        \n",
    "        # This class was created with the help of generative AI.\n",
    "        # Source OpenAI\n",
    "        # URL: https://chat.openai.com/\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        train_vectors, test_vectors, train_labels, test_labels = train_test_split(features, target, test_size = test_size, random_state=42)\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(train_vectors)\n",
    "        X_test_scaled = scaler.transform(test_vectors)\n",
    "\n",
    "        # Normalize the target variable\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        y_train_scaled = self.target_scaler.fit_transform(train_labels.values.reshape(-1, 1))\n",
    "        y_test_scaled = self.target_scaler.transform(test_labels.values.reshape(-1, 1))\n",
    "\n",
    "        # Set up the pca object with the number of components we want to find\n",
    "        pca = PCA(n_components=self.n_components, whiten=True)\n",
    "\n",
    "        # Fit the training data to the pca model.\n",
    "        X_fit = pca.fit(X_train_scaled)\n",
    "\n",
    "        total_variance = np.sum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "        # Transforming the vectors\n",
    "        pca_train_vectors = pca.transform(X_train_scaled)\n",
    "        pca_test_vectors = pca.transform(X_test_scaled)\n",
    "    \n",
    "        return pca_train_vectors, pca_test_vectors, y_train_scaled, y_test_scaled\n",
    "\n",
    "    def model(self, pca_train_vectors, pca_test_vectors, y_train_scaled, y_test_scaled):\n",
    "        \n",
    "        \"\"\"Models the dataset using TensorFlow.\n",
    "        \n",
    "        Arguements:\n",
    "        \n",
    "            pca_train_vectors [type: numpy array]\n",
    "                An array that the method process_data returns. \n",
    "                An array of scaled an fitted feature values ready to be acted upon by the model.\n",
    "                    \n",
    "            pca_test_vectors [type: numpy array]\n",
    "                An array that the method process_data returns. \n",
    "                An array of scaled an fitted feature values ready to be used to test the models accuracy.\n",
    "                \n",
    "            y_train_scaled [type: numpy array]\n",
    "                An array that the method process_data returns. \n",
    "                An array of scaled an fitted target values ready to be acted upon by the model.\n",
    "            \n",
    "            \n",
    "            y_test_scaled [type: numpy array]\n",
    "                An array that the method process_data returns. \n",
    "                An array of scaled an fitted feature values ready to be used to test the models accuracy.\"\"\"\\\n",
    "        \n",
    "        # The base code used to create the model was generated by CHAT GPT.\n",
    "        # Source OpenAI \n",
    "        # URL: https://chat.openai.com/\n",
    "        # In order to get the correct code working a long series of prompts was required over multiple days of work.\n",
    "        # For the sake of neatness I will not list the prompts here.\n",
    "        # The general prompts were,\n",
    "        # Can you generate a TensorFlow model to help me model NHL data?\n",
    "        # Fix this error.\n",
    "        # What other options do I have to improve accuracy?\n",
    "                \n",
    "        # Create the model\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(self.n_components,)),  # Input layer\n",
    "            tf.keras.layers.Dense(128, activation=self.activation_function, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(64, activation=self.activation_function, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.6),\n",
    "            tf.keras.layers.Dense(32, activation=self.activation_function, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(1)  # Output layer with one neuron for regression\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(pca_train_vectors, y_train_scaled, epochs=self.epoch, verbose=self.verbose)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred_scaled = model.predict(pca_test_vectors).flatten()\n",
    "\n",
    "        # Transform the predictions back to the original scale\n",
    "        y_test_pred = self.target_scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Inverse transform the true values back to the original scale\n",
    "        y_test_true = self.target_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Convert to binary classification using the threshold\n",
    "        y_test_pred_binary = (y_test_pred >= 0.35).astype(int)\n",
    "\n",
    "        # Calculate R-squared on the original scale\n",
    "        r2 = r2_score(y_test_true, y_test_pred_binary)\n",
    "        \n",
    "        # Returns the actual target, the predicted target, and the R-squared value.\n",
    "        return y_test_true, y_test_pred_binary, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c8e14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data \n",
    "df = pd.read_csv('Stanley Cup Data - project_dataset.csv') # This is the dataset from the NHL Website \n",
    "\n",
    "# Sorting the NHL data by year \n",
    "df = df.sort_values(by = \"Season Start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "147b046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (independent variables) and the target (dependent variable) for normal dataset\n",
    "features = df.drop(columns=['Season Start','Season End','Stanley Cup','Team','T','OT'])\n",
    "target = df['Stanley Cup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09fb935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model data for normal dataset\n",
    "modeled_data = DataAnalyzer(df, 15, 0.01, 'sigmoid', features, target, 100, 0) # Initialize the class\n",
    "\n",
    "pca_train_vectors_s, pca_test_vectors_s, y_train_scaled_s, y_test_scaled_s = modeled_data.process_data(features, target, 15, 0.25) # Process the dataset given\n",
    "\n",
    "y_stanley, y_prediction_stanley, r2_stanley = modeled_data.model(pca_train_vectors_s, pca_test_vectors_s, y_train_scaled_s, y_test_scaled_s) # Create a model and return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d1f24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared value for the normal data is -0.0342857142857147\n"
     ]
    }
   ],
   "source": [
    "print(f'The R-squared value for the stanley cup data is {r2_stanley}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed93b238",
   "metadata": {},
   "source": [
    "It is clear that with our given dataset and approach with TensorFlow that we are not going to be able to find a suitable R-Squared value. However we can still try to predict some other categories such as the presidents cup. The presidents cup is awarded to the team with the most points at the end of the season. Since points are directly correlated with wins TensorFlow should do a much better job at finding a suffcient R-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd35e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select new features (independent variables) and the target (dependent variable) for normal dataset\n",
    "new_features = df.drop(columns=['Season Start', 'Season End','Presidents Cup','Team','T','OT', 'GP'])\n",
    "new_target = df['Presidents Cup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3935a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model data for new features and target\n",
    "presidents_model = DataAnalyzer(df, 18, 0.01, 'sigmoid', new_features, new_target, 100, 0) # Initialize the class\n",
    "\n",
    "pca_train_vectors_p, pca_test_vectors_p, y_train_scaled_p, y_test_scaled_p = presidents_model.process_data(new_features, new_target, 15, 0.25) # Process the dataset given\n",
    "\n",
    "y_presidents, y_prediction_presidents, r2_presidents = presidents_model.model(pca_train_vectors_p, pca_test_vectors_p, y_train_scaled_p, y_test_scaled_p) # Create a model and return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94290f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 169\n",
      "False Postives: 10\n",
      "False Negatives: 1\n",
      "True Postives: 1\n",
      "The accuracy is 93.92265193370166\n"
     ]
    }
   ],
   "source": [
    "# Base code obtained from Sklearn documentation\n",
    "# Accessed at 10:40 AM 11/30/2023\n",
    "# URL: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_presidents, y_prediction_presidents,).ravel()\n",
    "print(f'True Negatives: {tn}\\nFalse Postives: {fp}\\nFalse Negatives: {fn}\\nTrue Postives: {tp}')\n",
    "\n",
    "accuracy = (tn+tp)/(tn+tp+fn+fp) *100\n",
    "print(f'The accuracy is {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a8044",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "https://datatofish.com/read_excel/\n",
    "\n",
    "https://www.nhl.com/stats/teams?aggregate=0&reportType=season&seasonFrom=19171918&seasonTo=20232024&gameType=2&filter=gamesPlayed,gte,1&sort=faceoffWinPct&page=0&pageSize=50\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html\n",
    "\n",
    "https://www.nhl.com/news/nhl-stanley-cup-champions-winners-complete-list-287705398\n",
    "\n",
    "https://python-charts.com/seaborn/themes/#:~:text=The%20seaborn%20library%20provides%20five,the%20set_style%20or%20set_style%20function.\n",
    "\n",
    "https://datatofish.com/csv-to-excel-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
